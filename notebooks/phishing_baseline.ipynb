{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "phishing_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phishing + AI — End-to-end Lab\n",
        "\n",
        "This notebook walks through dataset acquisition (fallback to repo sample), preprocessing, simple URL feature engineering, RandomForest baseline training, evaluation (confusion matrix, precision/recall/F1, PR-AUC), and SHAP-based interpretation.\n",
        "\n",
        "Notes:\n",
        "- If you want the full Kaggle dataset, upload `kaggle.json` and uncomment the Kaggle cell.\n",
        "- The notebook uses a small sample CSV included in the repo as fallback."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "colab": { "base_uri": "https://localhost:8080/" } },
      "source": [
        "# Install dependencies (run once in Colab)\n",
        "!pip install -q scikit-learn pandas matplotlib seaborn shap joblib\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Data acquisition\n",
        "Try to load a local `/content/phishing.csv`. If it does not exist, download the sample CSV from the repo raw URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "DATA_LOCAL = '/content/phishing.csv'\n",
        "if os.path.exists(DATA_LOCAL):\n",
        "    df = pd.read_csv(DATA_LOCAL)\n",
        "    print('Loaded local /content/phishing.csv')\n",
        "else:\n",
        "    # Fallback: raw sample CSV in this repo (adjust OWNER/repo if needed)\n",
        "    sample_url = 'https://raw.githubusercontent.com/anesra/phishing-baseline/main/data/sample/phishing_sample.csv'\n",
        "    try:\n",
        "        df = pd.read_csv(sample_url)\n",
        "        print('Loaded sample CSV from repo')\n",
        "    except Exception as e:\n",
        "        print('Could not download sample CSV automatically; please upload a CSV to /content/phishing.csv or provide Kaggle credentials.')\n",
        "        raise\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Quick EDA\n",
        "Check basic structure, label balance, and missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('shape:', df.shape)\n",
        "print('\\ncolumns:', df.columns.tolist())\n",
        "print('\\nlabel value counts:')\n",
        "if 'label' in df.columns:\n",
        "    print(df['label'].value_counts(dropna=False))\n",
        "else:\n",
        "    print('No column named \"label\" found — please map your label column to \"label\"')\n",
        "print('\\nmissing per column:')\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Preprocessing pipeline and simple feature engineering\n",
        "The sample CSV contains columns: URL, HTTPS_Token, URL_Length, having_At_Symbol, Prefix_Suffix, Shortening_Service, label\n",
        "We will extract/ensure a few numeric features and build a ColumnTransformer pipeline. If your dataset columns differ, adapt the column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from urllib.parse import urlparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def extract_url_features(url):\n",
        "    try:\n",
        "        s = str(url)\n",
        "        p = urlparse(s)\n",
        "        host = p.netloc or ''\n",
        "        path = p.path or ''\n",
        "    except Exception:\n",
        "        host, path, s = '', '', str(url)\n",
        "    return {\n",
        "        'url_length': len(s),\n",
        "        'host_length': len(host),\n",
        "        'path_length': len(path),\n",
        "        'has_https_token': int('https' in s.lower()),\n",
        "        'has_at_symbol': int('@' in s),\n",
        "        'num_dashes': s.count('-')\n",
        "    }\n",
        "\n",
        "# Apply feature extraction only if there's a URL column\n",
        "if 'URL' in df.columns:\n",
        "    features_df = df['URL'].apply(lambda u: pd.Series(extract_url_features(u)))\n",
        "    # merge into df\n",
        "    df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n",
        "else:\n",
        "    print('No URL column found — ensure your dataset has a URL column or provide feature columns directly')\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build preprocessing pipeline for the example feature set\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "num_cols = ['url_length','host_length','path_length','num_dashes']\n",
        "bin_cols = ['HTTPS_Token','has_https_token','having_At_Symbol','has_at_symbol','Prefix_Suffix','Shortening_Service']\n",
        "num_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
        "cat_transformer = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_transformer, num_cols),\n",
        "    ('cat', cat_transformer, bin_cols)\n",
        "])\n",
        "print('Preprocessor prepared with numeric and categorical transformers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Train / test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "if 'label' not in df.columns:\n",
        "    raise ValueError('No label column found. Please ensure your dataset has a \"label\" column with 1=phishing, 0=safe.')\n",
        "X = df.drop(columns=['label'])\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "print('Train:', X_train.shape, 'Test:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Baseline training (RandomForest)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "clf = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
        "])\n",
        "clf.fit(X_train, y_train)\n",
        "print('Baseline RandomForest trained')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
        "import seaborn as sns, matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.show()\n",
        "\n",
        "y_scores = clf.predict_proba(X_test)[:,1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "print('PR-AUC:', auc(recall, precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Interpretation with SHAP (sampled for performance)\n",
        "Compute SHAP values on a small sample for efficiency. If you run into memory issues, use fewer samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# For tree models use TreeExplainer\n",
        "explainer = shap.TreeExplainer(clf.named_steps['rf'])\n",
        "\n",
        "# sample rows to explain\n",
        "n_sample = min(200, len(X_test))\n",
        "X_sample = X_test.sample(n_sample, random_state=42)\n",
        "X_sample_pre = preprocessor.transform(X_sample)\n",
        "shap_values = explainer.shap_values(X_sample_pre)\n",
        "\n",
        "# Summary plot (class 1 = phishing)\n",
        "shap.summary_plot(shap_values[1], X_sample_pre, show=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Save artifacts (model + metrics)\n",
        "Save a trained model and metrics for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import joblib, json, os\n",
        "os.makedirs('experiments/run_01/artifacts', exist_ok=True)\n",
        "joblib.dump(clf, 'experiments/run_01/artifacts/phishing_rf_v1.joblib')\n",
        "metrics = {'pr_auc': float(auc(recall, precision))}\n",
        "with open('experiments/run_01/metrics.json','w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print('Saved model and metrics to experiments/run_01/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps / notes\n",
        "- Tune hyperparameters (RandomizedSearchCV, Optuna) optimizing PR-AUC or F1 according to operational needs.\n",
        "- Run SHAP on specific false positive / false negative examples to investigate causes.\n",
        "- If you have the full Kaggle dataset and `kaggle.json`, uncomment & run the Kaggle download cell (not included by default here).\n",
        "- For productionization: containerize the inference code and expose a small REST API (FastAPI) that returns probabilities and top explanation features.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
